[
    {
        "id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "name": "TinyLlama 1.1B (Chat)",
        "size": "2.2GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-1B-Instruct-4bit",
        "name": "Llama 3.2 1B Instruct (4-bit)",
        "size": "0.7GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-1B-Instruct-8bit",
        "name": "Llama 3.2 1B Instruct (8-bit)",
        "size": "1.3GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-1B-Instruct-bf16",
        "name": "Llama 3.2 1B Instruct (bf16)",
        "size": "2.6GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-3B-Instruct-4bit",
        "name": "Llama 3.2 3B Instruct (4-bit)",
        "size": "1.9GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-3B-Instruct-8bit",
        "name": "Llama 3.2 3B Instruct (8-bit)",
        "size": "3.4GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Llama-3.2-3B-Instruct-bf16",
        "name": "Llama 3.2 3B Instruct (bf16)",
        "size": "6.6GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit",
        "name": "Llama 3.1 8B Instruct (4-bit)",
        "size": "4.9GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Meta-Llama-3.1-8B-Instruct-8bit",
        "name": "Llama 3.1 8B Instruct (8-bit)",
        "size": "8.5GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Meta-Llama-3.1-70B-Instruct-4bit",
        "name": "Llama 3.1 70B Instruct (4-bit)",
        "size": "40GB",
        "family": "Llama"
    },
    {
        "id": "mlx-community/Qwen2.5-0.5B-Instruct-4bit",
        "name": "Qwen 2.5 0.5B Instruct (4-bit)",
        "size": "0.4GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-1.5B-Instruct-4bit",
        "name": "Qwen 2.5 1.5B Instruct (4-bit)",
        "size": "1.0GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-3B-Instruct-4bit",
        "name": "Qwen 2.5 3B Instruct (4-bit)",
        "size": "1.9GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-3B-Instruct-8bit",
        "name": "Qwen 2.5 3B Instruct (8-bit)",
        "size": "3.5GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-3B-Instruct-bf16",
        "name": "Qwen 2.5 3B Instruct (bf16)",
        "size": "7.0GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-7B-Instruct-4bit",
        "name": "Qwen 2.5 7B Instruct (4-bit)",
        "size": "4.5GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-7B-Instruct-8bit",
        "name": "Qwen 2.5 7B Instruct (8-bit)",
        "size": "8.0GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-14B-Instruct-4bit",
        "name": "Qwen 2.5 14B Instruct (4-bit)",
        "size": "9.0GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-32B-Instruct-4bit",
        "name": "Qwen 2.5 32B Instruct (4-bit)",
        "size": "19GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/Qwen2.5-72B-Instruct-4bit",
        "name": "Qwen 2.5 72B Instruct (4-bit)",
        "size": "41GB",
        "family": "Qwen"
    },
    {
        "id": "mlx-community/gemma-2-2b-it-4bit",
        "name": "Gemma 2 2B Instruct (4-bit)",
        "size": "1.5GB",
        "family": "Gemma"
    },
    {
        "id": "mlx-community/gemma-2-2b-it-8bit",
        "name": "Gemma 2 2B Instruct (8-bit)",
        "size": "2.8GB",
        "family": "Gemma"
    },
    {
        "id": "mlx-community/gemma-2-9b-it-4bit",
        "name": "Gemma 2 9B Instruct (4-bit)",
        "size": "5.5GB",
        "family": "Gemma"
    },
    {
        "id": "mlx-community/gemma-2-9b-it-8bit",
        "name": "Gemma 2 9B Instruct (8-bit)",
        "size": "10GB",
        "family": "Gemma"
    },
    {
        "id": "mlx-community/gemma-2-27b-it-4bit",
        "name": "Gemma 2 27B Instruct (4-bit)",
        "size": "16GB",
        "family": "Gemma"
    },
    {
        "id": "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
        "name": "Mistral 7B Instruct v0.3 (4-bit)",
        "size": "4.1GB",
        "family": "Mistral"
    },
    {
        "id": "mlx-community/Mistral-7B-Instruct-v0.3-8bit",
        "name": "Mistral 7B Instruct v0.3 (8-bit)",
        "size": "7.7GB",
        "family": "Mistral"
    },
    {
        "id": "mlx-community/Mixtral-8x7B-Instruct-v0.1-4bit",
        "name": "Mixtral 8x7B Instruct (4-bit)",
        "size": "24GB",
        "family": "Mistral"
    },
    {
        "id": "mlx-community/Mixtral-8x22B-Instruct-v0.1-4bit",
        "name": "Mixtral 8x22B Instruct (4-bit)",
        "size": "80GB",
        "family": "Mistral"
    },
    {
        "id": "mlx-community/Phi-3.5-mini-instruct-4bit",
        "name": "Phi 3.5 Mini Instruct (4-bit)",
        "size": "2.1GB",
        "family": "Phi"
    },
    {
        "id": "mlx-community/Phi-3.5-mini-instruct-8bit",
        "name": "Phi 3.5 Mini Instruct (8-bit)",
        "size": "4.0GB",
        "family": "Phi"
    },
    {
        "id": "mlx-community/DeepSeek-Coder-V2-Lite-Instruct-4bit",
        "name": "DeepSeek Coder V2 Lite (4-bit)",
        "size": "8.9GB",
        "family": "DeepSeek"
    },
    {
        "id": "mlx-community/DeepSeek-V3-4bit",
        "name": "DeepSeek V3 (4-bit)",
        "size": "300GB+",
        "family": "DeepSeek"
    },
    {
        "id": "ft-bc8cb33d-f857-4467-b7ce-577217192d65",
        "name": "Job12",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "adapter_path": "adapters/bc8cb33d-f857-4467-b7ce-577217192d65",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 100,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.001,
            "max_seq_len": 512,
            "dropout": 0.0
        }
    },
    {
        "id": "ft-f7ea7e7a-af19-45c8-acaa-b9050102b89a",
        "name": "Job14",
        "base_model": "mlx-community/Qwen2.5-0.5B-Instruct-4bit",
        "adapter_path": "adapters/f7ea7e7a-af19-45c8-acaa-b9050102b89a",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 6,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0004,
            "max_seq_len": 512,
            "dropout": 0.0
        }
    },
    {
        "id": "ft-f5ec6988-9524-485b-ba33-841ffe1d75e1",
        "name": "Job15",
        "base_model": "mlx-community/gemma-2-9b-it-4bit",
        "adapter_path": "adapters/f5ec6988-9524-485b-ba33-841ffe1d75e1",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": -1,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0
        }
    },
    {
        "id": "ft-1a4c2f97-738b-45bd-a51e-045b3c73e4f8",
        "name": "Job15",
        "base_model": "mlx-community/gemma-2-9b-it-4bit",
        "adapter_path": "adapters/1a4c2f97-738b-45bd-a51e-045b3c73e4f8",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 3,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0
        }
    },
    {
        "id": "ft-b4300eca-f798-40be-bcf5-a6f9267f5584",
        "name": "Test10",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "adapter_path": "adapters/b4300eca-f798-40be-bcf5-a6f9267f5584",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 6,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0
        }
    },
    {
        "id": "ft-6a712dfb-9ee6-4af4-bff5-f85170553aeb",
        "name": "Jon30",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "adapter_path": "adapters/6a712dfb-9ee6-4af4-bff5-f85170553aeb",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 6,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0,
            "lora_layers": 7
        }
    },
    {
        "id": "ft-1d54cb56-d6a0-44dd-836c-eddbac898510",
        "name": "Job20",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        "adapter_path": "adapters/1d54cb56-d6a0-44dd-836c-eddbac898510",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 5,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0,
            "lora_layers": 8
        }
    },
    {
        "id": "ft-e2f05d67-fb95-4a02-b3e0-179101214881",
        "name": "SyntheticTest",
        "base_model": "mlx-community/gemma-2-2b-it-4bit",
        "adapter_path": "adapters/e2f05d67-fb95-4a02-b3e0-179101214881",
        "size": "Adapter",
        "family": "Custom",
        "is_custom": true,
        "is_finetuned": true,
        "params": {
            "epochs": 10,
            "batch_size": 1,
            "lora_rank": 8,
            "lora_alpha": 16.0,
            "learning_rate": 0.0001,
            "max_seq_len": 512,
            "dropout": 0.0,
            "lora_layers": 8
        }
    }
]